{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# from model import vit_base_patch16_224\n",
    "# from timm.models import vit_base_patch16_224\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from saliency import *\n",
    "from utils import *\n",
    "from plots import *\n",
    "from run import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "model_path = '/home/raza.imam/Documents/HC701B/Project/models/vit_base_patch16_224_in21k_test-accuracy_0.96_chest.pth'\n",
    "device = 'cuda'\n",
    "attack = 'all' #default='all', choices=['FGSM', \"PGD\", \"all\"]\n",
    "train_path = \"/home/raza.imam/Documents/HC701B/Project/data/TB_data/training/\"\n",
    "num_train_imgs = 1000\n",
    "test_path = \"/home/raza.imam/Documents/HC701B/Project/data/TB_data/testing/\"\n",
    "dataset_class = \"Tuberculosis\" #default=\"Normal\", choices=[\"Tuberculosis\", \"Normal\"]\n",
    "block = -1 #last block\n",
    "eps = 0.02\n",
    "force_recompute = False\n",
    "num_test_imgs = 100\n",
    "\n",
    "# Additional constants\n",
    "ATTACK_LIST = [\"PGD\"]\n",
    "\n",
    "# Get the model\n",
    "model = get_model(model_path=model_path, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if reference images exist\n",
    "exits = False\n",
    "\n",
    "if exits:\n",
    "    mean_attns = {}\n",
    "    for attack_name in ATTACK_LIST + ['clean']:\n",
    "        mean_attns[attack_name] = np.load(\n",
    "            os.path.join(\n",
    "                \"./reference\",\n",
    "                \"mean_images\",\n",
    "                f\"mean_attns_{attack_name}_block_{block}_images_{num_train_imgs}.npy\"\n",
    "            )\n",
    "        )\n",
    "    print(f'Loaded mean images from ./reference/mean_images/')\n",
    "\n",
    "# Calculate reference images if they don't exist\n",
    "if not exits:\n",
    "    all_attns, mean_attns, mean_attn_diff = get_reference_attn_matp(\n",
    "        model=model,\n",
    "        image_folder=os.path.join(train_path, dataset_class),\n",
    "        block=block,\n",
    "        n_images=num_train_imgs,\n",
    "        device=device,\n",
    "        attack_type=ATTACK_LIST,\n",
    "        select_random=False,\n",
    "    )\n",
    "\n",
    "    # Save mean images to disk\n",
    "    os.makedirs(os.path.join(\"./reference\", \"mean_images\"), exist_ok=True)\n",
    "    print(f'Saving mean images to ./reference/mean_images/')\n",
    "    for attack_name in ATTACK_LIST + ['clean']:\n",
    "        np.save(\n",
    "            os.path.join(\n",
    "                \"./reference\",\n",
    "                \"mean_images\",\n",
    "                f\"mean_attns_{attack_name}_block_{block}_images_{num_train_imgs}.npy\"\n",
    "            ),\n",
    "            mean_attns[attack_name]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_attns['clean']), mean_attns['clean'].shape, mean_attns['PGD'].shape, mean_attn_diff['PGD'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test images and attention maps\n",
    "test_imgs, test_attns, test_attn_diff, test_img_files = get_images_attns(\n",
    "    model=model,\n",
    "    image_folder=os.path.join(test_path, dataset_class),\n",
    "    n_imgs=num_test_imgs,\n",
    "    block=block,\n",
    "    device=device,\n",
    "    attack_type=ATTACK_LIST,\n",
    "    eps=eps,\n",
    "    plot=False,\n",
    "    rand=False,\n",
    "    random_state=None,\n",
    ")\n",
    "\n",
    "num_test_images = len(test_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "text = [\"Original Image\", \"Head Mean Clean\", \"Head Mean Adv\"]\n",
    "img_no = 14\n",
    "for i, fig in enumerate([test_imgs[img_no].squeeze(0).permute(2,1,0), test_attns[img_no]['clean'], test_attns[img_no]['PGD']]):\n",
    "    print(fig.shape)\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.imshow(fig, cmap='inferno')\n",
    "    plt.title(text[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for predictions\n",
    "img_name = []\n",
    "gt_labels = []\n",
    "sum_preds = []\n",
    "euc_preds = []\n",
    "cos_preds = []\n",
    "\n",
    "# Test each test image\n",
    "for idx, attn_map in enumerate(test_attns):\n",
    "    for i, attack_name in enumerate(ATTACK_LIST + ['clean']):\n",
    "        result = classify_image(\n",
    "            img_attn_map=attn_map[attack_name],\n",
    "            mean_attn_clean=mean_attns['clean'],\n",
    "            mean_attn_adv=mean_attns['PGD']\n",
    "        )\n",
    "        sum_preds.append(result['sum'])\n",
    "        euc_preds.append(result['euclidean'])\n",
    "        cos_preds.append(result['cosine'])\n",
    "        gt_labels.append(attack_name)\n",
    "        img_name.append(test_img_files[idx])\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_dict = {\n",
    "    \"image\": img_name,\n",
    "    \"GT\": gt_labels,\n",
    "    \"sum\": sum_preds,\n",
    "    \"euclidean\": euc_preds,\n",
    "    \"cosine\": cos_preds,\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "os.makedirs(\"./results\", exist_ok=True)\n",
    "\n",
    "# Save results to a CSV file\n",
    "results_df.to_csv(\n",
    "    f\"./results/preds_{dataset_class}_block_{block}_images_{num_train_imgs}_eps_{eps}.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Calculate binary ground truth labels (1 for clean, 0 for adversarial)\n",
    "gt_labels_bin = [1 if label == \"clean\" else 0 for label in gt_labels]\n",
    "\n",
    "methods = [\"sum\", \"euclidean\", \"cosine\"]\n",
    "\n",
    "# Evaluate and print results\n",
    "for method in methods:\n",
    "    pred_bin = [1 if label == \"Clean\" else 0 for label in results_dict[method]]\n",
    "    print(f'--------------- {method} ---------------')\n",
    "    print(f\"Accuracy for {method}: {accuracy_score(gt_labels_bin, pred_bin)}\")\n",
    "    print(f\"F1 score for {method}: {f1_score(gt_labels_bin, pred_bin)}\")\n",
    "    print(f'-------------------------------------------------------------------')\n",
    "    print(f'-------------------------------------------------------------------')\n",
    "\n",
    "# Log results to a file\n",
    "import datetime\n",
    "ct = datetime.datetime.now()\n",
    "with open(\"./logs.txt\", \"a\") as f:\n",
    "    print('', file=f)\n",
    "    print(f'--------------------------------------------- {ct} ---------------------------------------------\\n', file=f)\n",
    "    print(f'--------------------------Results-----------------------------\\n', file=f)\n",
    "    for method in methods:\n",
    "        pred_bin = [1 if label == \"Clean\" else 0 for label in results_dict[method]]\n",
    "        # Log to file\n",
    "        print(f'--------------- {method} ---------------', file=f)\n",
    "        print(f\"Accuracy for {method}: {accuracy_score(gt_labels_bin, pred_bin)}\", file=f)\n",
    "        print(f\"F1 score for {method}: {f1_score(gt_labels_bin, pred_bin)}\", file=f)\n",
    "        print(f'-------------------------------------------------------------------', file=f)\n",
    "\n",
    "        # Print to screen\n",
    "        print(f'--------------- {method} ---------------')\n",
    "        print(f\"Accuracy for {method}: {accuracy_score(gt_labels_bin, pred_bin)}\")\n",
    "        print(f\"F1 score for {method}: {f1_score(gt_labels_bin, pred_bin)}\")\n",
    "        print(f'-------------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xaim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
